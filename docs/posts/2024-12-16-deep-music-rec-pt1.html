<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="description" content="Marco Zocca" /> 
        <meta name="author" content="Marco Zocca" />
        <title>Marco Zocca - Deep music recommendation - pt. 1</title>

        <link rel="stylesheet" href="../css/default.css" />

        <link rel="stylesheet" href="../css/highlight_default.min.css">
        <script src="../js/highlight.min.js"></script>
        <script src="../js/haskell.min.js"></script>
        <script>hljs.highlightAll();</script>

        <script id="MathJax-script" async src="../js/mathjax.min.js"></script>
    </head>
    <body>


        <div class="container content">

        <header class="masthead">
            <div class="row">
              <div class="col-sm-4">
                    <h2 class="masthead-title">
                      <a href="../index.html" title="Home">Marco Zocca</a>
                </h2>
              </div>
              <div class="col-sm-8">
                    <h4 class="masthead-title" style="text-align: right;">
                      <a href="../about.html" title="About">About</a>
                  &nbsp;
                      <a href="../research.html" title="Research">Research</a>
                  &nbsp;
                      <a href="../oss.html" title="Open source">Open Source</a>
                      <!-- <a class='icon-github-squared' href='/oss.html' title='Open source'>Open Source</a> -->
                  &nbsp;
                      <a href="../archive.html">Posts</a>
                </h4>
              </div>
            </div>
              </header>


        <main role="main">
            <h1>Deep music recommendation - pt. 1</h1>
            <article class="post">
    <section class="header">
        December 16, 2024
    </section>
    <section>
        <h1 id="introduction">Introduction</h1>
<!-- ![Preference graph](prefs_graph.png "Preference graph") -->
<p>Can we learn a music recommendation model from raw audio samples and a preference graph?</p>
<p><img src="../images/prefs_graph.png" width="400/"></p>
<p>This project started from this question, and the curiosity to combine together a few topics I’ve been curious about recently: audio processing, contrastive learning and graphs.</p>
<h1 id="technical-summary">Technical summary</h1>
<p>This is an audio embedding model, trained to produce embeddings that are close if their respective graph distances are small. In other words, I used the preference graph as supervision labels for the audio embeddings: in a contrastive setting, we consider an “anchor” sample, a positive and a negative one, which are here distinguished by their graph distance to the anchor.</p>
<p>It becomes a (context-free, static, non-personalized) recommendation model by:</p>
<ul>
<li>embedding a music collection</li>
<li>storing it into an index (here I used SQLite)</li>
<li>embedding a query (a new music track) with the same model and looking up the most similar ones from the index.</li>
</ul>
<h1 id="dataset">Dataset</h1>
<p>In order to limit the size of the dataset I only considered music samples having the largest <a href="https://en.wikipedia.org/wiki/Centrality#Degree_centrality">in-degree centrality</a>, i.e. the largest number of inbound edges. In simpler words, these are the most recommended albums in the dataset.</p>
<p>Each graph vertex corresponds to a music /album/ which contains one or more tracks.</p>
<p>There are a number of preprocessing steps, and the intermediate results are stored in SQLite, indexed by album, track and chunk ID. For the sake of brevity let’s summarize the preprocessing:</p>
<ul>
<li>Compute the graph in-degrees from the edges: <code>INSERT OR REPLACE INTO nodes_degrees SELECT to_node, count(to_node) FROM edges GROUP BY to_node</code></li>
<li>Download top <span class="math inline"><em>k</em></span> albums by in-degree centrality: <code>SELECT album_url, nodes.album_id FROM nodes INNER JOIN nodes_degrees ON nodes.album_id = nodes_degrees.album_id WHERE degree &gt; {degree_min} ORDER BY degree DESC LIMIT {k}</code>. So far we used <span class="math inline"><em>k</em> = 50</span>.</li>
<li>For each track in each album: split the audio in 30-seconds chunks, and assign it to either the training or test or validation partition. It’s crucial to fix the chunk length, as PyTorch works with data batches, and each batch is a (anchor, positive, negative)-tuple of <span class="math inline"><em>B</em> × <em>T</em></span> tensors (batch size, time steps).</li>
<li>Compute the preference graph distances for each album</li>
<li>For each dataset partition and audio chunk, sample a few other chunks from the graph distance map (<a href="https://en.wikipedia.org/wiki/Isochrone_map">“isochrone”</a>?).</li>
</ul>
<p>The music preference graph and audio samples were constructed from public sources.</p>
<h1 id="model-take-1">Model, take 1</h1>
<p>I initially experimented with a <a href="https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html#torch_geometric.nn.conv.GCNConv">graph convolutional network</a>. The idea behind this was to:</p>
<ul>
<li>Embed the most central audio samples</li>
<li>Diffuse the embeddings out to all the remaining nodes with the GCN.</li>
</ul>
<p>I dropped this approach because even with 1 or 2 GCN layers it required an awful amount of memory, and is clearly wasteful because we need to begin with random or zero embeddings for all the nodes that don’t have audio attached.</p>
<h1 id="model-take-2">Model, take 2</h1>
<p>The embedding model is closely related to the <a href="https://sander.ai/2014/08/05/spotify-cnns.html">“Spotify CNN” introduced here back in 2014.</a></p>
<p>The NN architecture can be broken down as follows:</p>
<ul>
<li>the audio samples are first transformed into <b>mel-spectrograms</b> (which bins frequencies according to a human perceptual model);</li>
<li>the STFT representation is fed to 3 <b>convolutional stages</b>, i.e. <code>Conv1d</code> interleaved with a max-pooling operation (window size 4 and 2 respectively). Both the convolutions and the pooling are done over the time axis only.</li>
<li>After the last 1D convolution there is an <b>average pooling</b> operation over the whole time axis. The result of this is a vector having size <code>n_mels</code> for each sample.</li>
<li>Next, there are three <b>linear layers</b> interleaved by a <code>ReLU</code> nonlinearity. The first linear layer maps from <code>n_mels</code> to a larger <code>dim_hidden</code>, the middle one is a square matrix and the last one projects the hidden dimension down to our embedding space.</li>
<li>The fully-connected layers are then followed by a <span class="math inline"><em>L</em><sub>2</sub></span> <b>normalization</b> step.</li>
</ul>
<p>The main changes from the Spotify CNN are:</p>
<ul>
<li>I don’t use 3 different time pooling functions but only an average pooling.</li>
<li>The loss function: here I use a <a href="https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginLoss.html">triplet loss</a> based on the <a href="https://en.wikipedia.org/wiki/Cosine_similarity#Cosine_distance">cosine “distance”</a>, defined as:</li>
</ul>
<p><span class="math display">$$
d(x_1, x_2) := \sqrt{2 ( 1 - (x_1 \cdot x_2)) }
$$</span></p>
<p>(NB: the definition above assumes unit-norm vectors <span class="math inline"><em>x</em><sub>1</sub></span> and <span class="math inline"><em>x</em><sub>2</sub></span>).</p>
<h1 id="training">Training</h1>
<p><img src="../images/melspec_training_loss.png" width="400/"></p>
<p>With the following parameters:</p>
<ul>
<li>Adam optimizer</li>
<li>base learning rate : 0.005</li>
<li>batch size = 16</li>
</ul>
    </section>
</article>

        </main>

        <footer>
        </footer>

        </div>


    </body>
</html>
