<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="description" content="Marco Zocca" /> 
        <meta name="author" content="Marco Zocca" />
        <title>Marco Zocca - Building a deep learning-based music recommendation system - pt. 1</title>

        <link rel="stylesheet" href="../css/default.css" />

        <link rel="stylesheet" href="../css/highlight_default.min.css">
        <script src="../js/highlight.min.js"></script>
        <script src="../js/haskell.min.js"></script>
        <script>hljs.highlightAll();</script>

        <script id="MathJax-script" async src="../js/mathjax.min.js"></script>
    </head>
    <body>


        <div class="container content">

        <header class="masthead">
            <div class="row">
              <div class="col-sm-4">
                    <h2 class="masthead-title">
                      <a href="../index.html" title="Home">Marco Zocca</a>
                </h2>
              </div>
              <div class="col-sm-8">
                    <h4 class="masthead-title" style="text-align: right;">
                      <a href="../about.html" title="About">About</a>
                  &nbsp;
                      <a href="../research.html" title="Research">Research</a>
                  &nbsp;
                      <a href="../oss.html" title="Open source">Open Source</a>
                      <!-- <a class='icon-github-squared' href='/oss.html' title='Open source'>Open Source</a> -->
                  &nbsp;
                      <a href="../archive.html">Posts</a>
                </h4>
              </div>
            </div>
              </header>


        <main role="main">
            <h1>Building a deep learning-based music recommendation system - pt. 1</h1>
            <article class="post">
    <section class="header">
        December 16, 2024
    </section>
    <section>
        <h1 id="introduction">Introduction</h1>
<!-- ![Preference graph](prefs_graph.png "Preference graph") -->
<p>Can we learn a music recommendation model from raw audio samples and a preference graph?</p>
<p><img src="../images/prefs_graph.png" width="500/"></p>
<p>This project started from this question, and the curiosity to combine together a few topics I’ve been curious about recently: audio processing with deep neural networks, contrastive learning and graph data.</p>
<p>Not to mention the fact that I’m constantly looking for new music, and while friends and a few trusted broadcasters (e.g. <a href="https://nts.live">NTS</a>) are very helpful, I also rely on YouTube suggestions. So, what makes a good recommender model? Let’s find out!</p>
<h1 id="technical-summary">Technical summary</h1>
<p>This post describes an audio embedding model, trained to produce embeddings that are close together if the graph distances of their respective albums are small. In other words, I used the preference graph as supervision labels for the audio embeddings: in a contrastive setting, we consider an “anchor” sample, a positive and a negative one, which are here distinguished by their graph distance to the anchor.</p>
<p>It becomes a (context-free, static, non-personalized) recommendation model by:</p>
<ul>
<li>embedding a music collection with the trained model</li>
<li>storing it into an index (here I used SQLite)</li>
<li>embedding a query (a new music track) with the same model and looking up the most similar ones from the index.</li>
</ul>
<h1 id="dataset">Dataset</h1>
<p>In order to limit the size of the dataset I only considered music samples having the largest <a href="https://en.wikipedia.org/wiki/Centrality#Degree_centrality">in-degree centrality</a>, i.e. the largest number of inbound edges. In simpler words, these are the most recommended albums in the dataset.</p>
<p>Each graph vertex corresponds to a music /album/ which contains one or more tracks.</p>
<p>There are a number of preprocessing steps, and the intermediate results are stored in SQLite, indexed by album, track and chunk ID. For the sake of brevity let’s summarize the preprocessing:</p>
<ul>
<li>Compute the graph in-degrees from the edges: <code>INSERT OR REPLACE INTO nodes_degrees SELECT to_node, count(to_node) FROM edges GROUP BY to_node</code></li>
<li>Download top <span class="math inline"><em>k</em></span> albums by in-degree centrality: <code>SELECT album_url, nodes.album_id FROM nodes INNER JOIN nodes_degrees ON nodes.album_id = nodes_degrees.album_id WHERE degree &gt; {degree_min} ORDER BY degree DESC LIMIT {k}</code>. So far we used <span class="math inline"><em>d</em><em>e</em><em>g</em><em>r</em><em>e</em><em>e</em> = 10</span> and <span class="math inline"><em>k</em> = 50</span>.</li>
<li>For each track in each album: split the audio in 30-seconds chunks, and assign it to either the training or test or validation partition. It’s crucial to fix the chunk length, as training works with data batches, and each batch is a (anchor, positive, negative)-tuple of <span class="math inline"><em>B</em> × <em>T</em></span> tensors (batch size, time steps).</li>
<li>Compute the preference graph distances for each album, up to distance <span class="math inline"><em>d</em><sub><em>m</em><em>a</em><em>x</em></sub></span>, by breadth-first search. So far I used <span class="math inline"><em>d</em><sub><em>m</em><em>a</em><em>x</em></sub> = 4</span></li>
<li>For each dataset partition and audio chunk, sample a few other chunks from the graph distance map (<a href="https://en.wikipedia.org/wiki/Isochrone_map">“isochrone”</a>?), among the closest and farthest from the anchor. The IDs for these will be stored in a triplet metadata table</li>
<li>The PyTorch <code>Dataset</code> looks up a triplet from a row index, then using that it retrieves the respective audio chunks (which are stored in SQLite as <code>np.ndarray</code>s).</li>
</ul>
<p>The music preference graph and audio samples were constructed from public sources.</p>
<h1 id="model-take-1">Model, take 1</h1>
<p>I initially experimented with a <a href="https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html#torch_geometric.nn.conv.GCNConv">graph convolutional network</a>. The idea behind this was to:</p>
<ul>
<li>Embed the most central audio samples</li>
<li>Diffuse the embeddings out to all the remaining nodes with the GCN.</li>
</ul>
<p>I dropped this approach because even with 1 GCN layer it required an awful amount of memory (PyTorch crashed on a 16 GB vRAM T4 GPU instance), and initializing embeddings of all the nodes that don’t have audio attached require a whole set of dedicated experiments which I didn’t have time for.</p>
<h1 id="model-take-2">Model, take 2</h1>
<p>The embedding model is similar to the <a href="https://sander.ai/2014/08/05/spotify-cnns.html">“Spotify CNN” introduced here back in 2014.</a></p>
<p>The NN architecture can be broken down as follows:</p>
<ul>
<li>the audio samples are first transformed into <b>mel-spectrograms</b> (which bins frequencies according to a human perceptual model). I use <code>n_mels = 128</code>, 2048 FFT samples and a FFT stride of 1024 throughout.</li>
<li>the STFT representation is fed to 3 <b>convolutional stages</b>, i.e. <code>Conv1d</code> interleaved with a max-pooling operation (window size 4 and 2 respectively). Both the convolutions and the pooling are done over the time axis only.</li>
<li>After the last 1D convolution there is an <b>average pooling</b> operation over the whole time axis. The result of this is a vector having size <code>n_mels</code> for each sample.</li>
<li>Next, there are three <b>linear layers</b> interleaved with a <code>ReLU</code> nonlinearity. The first linear layer maps from <code>n_mels</code> to a larger <code>dim_hidden = 1024</code>, the middle one is a square matrix and the last one projects the hidden dimension down to our embedding space.</li>
<li>The fully-connected layers are then followed by a <span class="math inline"><em>L</em><sub>2</sub></span> <b>normalization</b> step.</li>
</ul>
<p>The main changes from the Spotify CNN are:</p>
<ul>
<li>I don’t use 3 different time pooling functions but only an average pooling.</li>
<li>The loss function: here I use a <a href="https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginLoss.html">triplet loss</a> based on the <a href="https://en.wikipedia.org/wiki/Cosine_similarity#Cosine_distance">Euclidean distance</a>, defined as:</li>
</ul>
<p><span class="math display">$$
d(x_1, x_2) := \sqrt{2 ( 1 - (x_1 \cdot x_2)) }
$$</span></p>
<p>(NB: the definition above assumes unit-norm vectors <span class="math inline"><em>x</em><sub>1</sub></span> and <span class="math inline"><em>x</em><sub>2</sub></span>).</p>
<h1 id="training">Training</h1>
<p>Training the model above converges quite smoothly, as we can see below:</p>
<p><img src="../images/melspec_training_loss.png" width="500/"></p>
<p>With the following parameters:</p>
<ul>
<li>Adam optimizer</li>
<li>base learning rate = 0.005</li>
<li>batch size = 16</li>
</ul>
<p>This does not use any form of data augmentation, and interestingly the validation loss seems to keep slowly decreasing even after a large number of epochs.</p>
<h1 id="saving-checkpoints-for-inference">Saving checkpoints for inference</h1>
<p>I use <a href="https://lightning.ai/docs/pytorch/stable/">PyTorch Lightning</a> for all my deep learning models, which takes care of automatically saving models during and at the end of each training run.</p>
<p>Initially I planned to export the models to ONNX for faster inference but it turns out at least one of my model blocks (the mel-spectrogram FFT) is not currently supported by ONNX due to some missing complex number implementation <code>:/</code></p>
    </section>
</article>

        </main>

        <footer>
        </footer>

        </div>


    </body>
</html>
