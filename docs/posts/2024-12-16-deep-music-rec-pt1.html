<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="description" content="Marco Zocca" /> 
        <meta name="author" content="Marco Zocca" />
        <title>Marco Zocca - Reconstructing a music recommendation model</title>

        <link rel="stylesheet" href="../css/default.css" />

        <link rel="stylesheet" href="../css/highlight_default.min.css">
        <script src="../js/highlight.min.js"></script>
        <script src="../js/haskell.min.js"></script>
        <script>hljs.highlightAll();</script>

        <script id="MathJax-script" async src="../js/mathjax.min.js"></script>
    </head>
    <body>


        <div class="container content">

        <header class="masthead">
            <div class="row">
              <div class="col-sm-4">
                    <h2 class="masthead-title">
                      <a href="../index.html" title="Home">Marco Zocca</a>
                </h2>
              </div>
              <div class="col-sm-8">
                    <h4 class="masthead-title" style="text-align: right;">
                      <a href="../about.html" title="About">About</a>
                  &nbsp;
                      <a href="../research.html" title="Research">Research</a>
                  &nbsp;
                      <a href="../oss.html" title="Open source">Open Source</a>
                      <!-- <a class='icon-github-squared' href='/oss.html' title='Open source'>Open Source</a> -->
                  &nbsp;
                      <a href="../archive.html">Posts</a>
                </h4>
              </div>
            </div>
              </header>


        <main role="main">
            <h1>Reconstructing a music recommendation model</h1>
            <article class="post">
    <section class="header">
        December 16, 2024
    </section>
    <section>
        <h1 id="introduction">Introduction</h1>
<!-- ![Preference graph](prefs_graph.png "Preference graph") -->
<p>Can we learn a music recommendation model from raw audio samples and a preference graph?</p>
<p><img src="../images/prefs_graph.png" width="500/"></p>
<p>To answer this, I wanted to combine together a few topics I’ve been curious about recently: audio processing with deep neural networks, contrastive learning and graph data.</p>
<p>Not to mention the fact that I’m constantly looking for new music, and while friends and a few trusted broadcasters (e.g. <a href="https://nts.live">NTS</a>) are very helpful, I also rely on YouTube suggestions. So, what makes a good recommender model? Let’s find out!</p>
<h1 id="technical-summary">Technical summary</h1>
<p>This post describes an audio embedding model, trained to produce embeddings that are close together if the graph distances of their respective albums are small. In other words, I used the preference graph as supervision labels for the audio embeddings: in a contrastive setting, we consider an “anchor” sample, a positive and a negative one, which are here distinguished by their graph distance to the anchor.</p>
<p>It becomes a (context-free, static, non-personalized) recommendation model by:</p>
<ul>
<li>embedding a music collection with the trained model</li>
<li>storing it into an index (here I used SQLite)</li>
<li>embedding a query (a new music track) with the same model and looking up the most similar ones from the index.</li>
</ul>
<h1 id="dataset">Dataset</h1>
<p>The recommendations graph is taken as fixed, and we do not know how it was constructed (whether using content information, or user interactions, or both).</p>
<p>Each graph vertex corresponds to a music /album/ which contains one or more tracks. In order to limit the size of the audio dataset I only considered music albums having the largest <a href="https://en.wikipedia.org/wiki/Centrality#Degree_centrality">in-degree centrality</a>. In simpler words, these are the most recommended albums in the recommendations graph.</p>
<p>There are a number of preprocessing steps, and the intermediate results are stored in SQLite to minimize the amount of training-time CPU compute. Here is a summary:</p>
<ul>
<li>Compute the <b>graph in-degrees</b> : <code>INSERT INTO nodes_degrees SELECT to_node, count(to_node) FROM edges GROUP BY to_node</code></li>
<li>Download top <span class="math inline"><em>k</em></span> albums by in-degree <b>centrality</b>: <code>SELECT album_url, nodes.album_id FROM nodes INNER JOIN nodes_degrees ON nodes.album_id = nodes_degrees.album_id WHERE degree &gt; {degree_min} ORDER BY degree DESC LIMIT {k}</code>. So far I used <code>degree_min</code> = 10 and <code>k</code> = 50.</li>
<li>For each track in each album: split the audio in <b>30-seconds chunks</b>, and assign it to either the training or test or validation partition. It’s crucial to fix the chunk length, as training works with data batches, and each batch is a (anchor, positive, negative)-tuple of <span class="math inline"><em>B</em> × <em>T</em></span> tensors (batch size, time steps).</li>
<li>Compute the preference <b>graph distances</b> for each album, up to distance <span class="math inline"><em>d</em><sub><em>m</em><em>a</em><em>x</em></sub></span>, by breadth-first search. So far I used <span class="math inline"><em>d</em><sub><em>m</em><em>a</em><em>x</em></sub> = 4</span>.</li>
<li>For each dataset partition and audio chunk, sample a few other chunks from the graph distance map (<a href="https://en.wikipedia.org/wiki/Isochrone_map">“isochrone”</a>?), among the closest and farthest from the anchor. The IDs for these will be stored in a <b>triplet metadata table</b>.</li>
<li>The PyTorch <code>Dataset</code> looks up a triplet from a row index, then using that it retrieves the respective audio chunks (which are stored in SQLite as <code>np.ndarray</code>s).</li>
</ul>
<p>As a side note, I highly recommend storing intermediate dataset stages in a SQLite database rather than in a filesystem. This lets us look up things by various attributes without having to rely on crazy regexes, which in turn should help with long-term maintainability. Since it’s always available thanks to the Python base library, you don’t have to worry about the DB being unreachable and the like.</p>
<p>The music preference graph and audio samples were constructed from public sources.</p>
<h1 id="model-take-1">Model, take 1</h1>
<p>I initially experimented with a <a href="https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html#torch_geometric.nn.conv.GCNConv">graph convolutional network</a>. The idea behind this was to:</p>
<ul>
<li>Embed the most central audio samples</li>
<li>Diffuse the embeddings out to all the remaining nodes with the GCN.</li>
</ul>
<p>I dropped this approach because even with 1 GCN layer it required an awful amount of memory (PyTorch crashed with 16 GB vRAM).</p>
<h1 id="model-take-2">Model, take 2</h1>
<p>The embedding model is similar to the <a href="https://sander.ai/2014/08/05/spotify-cnns.html">“Spotify CNN” introduced here back in 2014, with a couple variations.</a></p>
<p>The NN architecture can be broken down as follows:</p>
<ul>
<li>the audio samples are first <b>down-sampled</b> at 16 kHz and transformed into <b>mel-spectrograms</b> (which bins frequencies according to a human perceptual model). I use <code>n_mels = 128</code>, 2048 FFT samples and a FFT stride of 1024 throughout.</li>
<li>the STFT representation is fed to 3 <b>convolutional stages</b>, i.e. <code>Conv1d</code> interleaved with a <b>max-pooling operation</b> (window size 4 and 2 respectively). Both the convolutions and the pooling are done over the time axis only.</li>
<li>After the last 1D convolution there is an <b>average pooling</b> operation over the whole time axis. The result of this is a vector of size <code>n_mels</code>.</li>
<li>Next, there are three <b>linear layers interleaved with a ReLU nonlinearity</b>. The first linear layer maps from <code>n_mels</code> to a larger <code>dim_hidden = 1024</code>, the middle one is a square matrix and the last one projects the hidden dimension down to our embedding space.</li>
<li>The fully-connected layers are then followed by a <span class="math inline"><em>L</em><sub>2</sub></span> <b>normalization</b> step.</li>
</ul>
<p>The main changes from the Spotify CNN are:</p>
<ul>
<li>I don’t use 3 different time pooling functions but only an average pooling.</li>
<li>The loss function: I use a <a href="https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginLoss.html">triplet loss</a> based on the <a href="https://en.wikipedia.org/wiki/Cosine_similarity#Cosine_distance">Euclidean distance</a>.</li>
</ul>
<h1 id="training">Training</h1>
<p>At peak, training the model takes less than 4 GB of vRAM, which sits comfortably within a T4 or similar cheap-tier GPU. It’s also pretty fast, at 10-12 batches per second.</p>
<p>Training the model above converges quite smoothly, as we can see below:</p>
<p><img src="../images/melspec_training_loss.png" width="500/"></p>
<p>With the following parameters:</p>
<ul>
<li>Adam optimizer</li>
<li>base learning rate = 0.005</li>
<li>batch size = 16</li>
</ul>
<p>This BTW does not use any form of data augmentation, and interestingly the validation loss seems to keep slowly decreasing even after a large number of epochs. I suppose one could search for better optimizer hparams (the validation loss jumps around quite a bit) but this looks already good enough.</p>
<h1 id="saving-checkpoints-for-inference">Saving checkpoints for inference</h1>
<p>I use <a href="https://lightning.ai/docs/pytorch/stable/">PyTorch Lightning</a> for all my deep learning models, which takes care of automatically saving models during and at the end of each training run.</p>
<p>Initially I planned to export the models to ONNX for faster inference but it turns out at least one of my model blocks (the mel-spectrogram FFT) is not currently supported by ONNX due to some missing complex number implementation <code>:/</code></p>
<h1 id="conclusion">Conclusion</h1>
<p>In this post I’ve shown a way to use a preference graph as supervision signal for training a neural audio embedding model.</p>
<p>Stay tuned for pt. 2 with evaluation and more! Thanks for reading!</p>
    </section>
</article>

        </main>

        <footer>
        </footer>

        </div>


    </body>
</html>
