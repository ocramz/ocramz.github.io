<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="description" content="Marco Zocca" /> 
        <meta name="author" content="Marco Zocca" />
        <title>Marco Zocca - Minimum bipartite matching via Riemann optimization</title>

        <link rel="stylesheet" href="../css/default.css" />

        <link rel="stylesheet" href="../css/highlight_default.min.css">
        <script src="../js/highlight.min.js"></script>
        <script src="../js/haskell.min.js"></script>
        <script>hljs.highlightAll();</script>

        <!--  <script id="MathJax-script" async src="/js/mathjax.min.js"></script> -->
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        
    </head>
    <body>


        <div class="container content">

        <header class="masthead">
            <div class="row">
              <div class="col-sm-4">
                    <h2 class="masthead-title">
                      <a href="../index.html" title="Home">Marco Zocca</a>
                </h2>
              </div>
              <div class="col-sm-8">
                    <h4 class="masthead-title" style="text-align: right;">
                      <a href="../about.html" title="About">About</a>
                  &nbsp;
                      <a href="../research.html" title="Research">Research</a>
                  &nbsp;
                      <a href="../oss.html" title="Open source">Open Source</a>
                      <!-- <a class='icon-github-squared' href='/oss.html' title='Open source'>Open Source</a> -->
                  &nbsp;
                      <a href="../archive.html">Posts</a>
                </h4>
              </div>
            </div>
              </header>


        <main role="main">
            <h1>Minimum bipartite matching via Riemann optimization</h1>
            <article class="post">
    <section class="header">
        December 21, 2023
    </section>
    <section>
        <h1 id="introduction">Introduction</h1>
<p><img src="../images/assignment_riemann_tri.png" /></p>
<p>Some time ago I ran into two separate instances of the same combinatorial optimization problem in the span of a few days, and decided to read up a little on the fundamentals. The two applications were object tracking in videos, and peaks alignment in chromatography data.</p>
<p>By chasing down some definitions, I learned about the Birkhoff theorem, which led me to wonder whether we can turn the original combinatorial problem (<a href="https://math.mit.edu/~goemans/18433S13/matching-notes.pdf">minimum weight bipartite matching <em>aka</em> ‚Äúassignment‚Äù</a> [0]) into a differentiable one. The other half of my investigation was about how to turn a constrained continuous optimization problem into an unconstrained one over an appropriate manifold.</p>
<p>In this post I document my experiments which started from a ‚Äúwhat if?‚Äù and ended up connecting some interesting areas of mathematics and computer science (and an empirical confirmation to my starting intuition, I should add).</p>
<h2 id="minimum-bipartite-matching">Minimum bipartite matching</h2>
<p>Given a bipartite graph between two sets <span class="math inline"><em>U</em></span>, <span class="math inline"><em>V</em></span> of <span class="math inline"><em>n</em></span> items each, and an edge cost matrix <span class="math inline"><em>C</em></span> with positive entries, the assignment problem can be written as finding a permutation matrix <span class="math inline"><em>P</em></span> that minimizes the total cost:</p>
<p><span class="math display">$$
P^{\star} = \underset{P \in \mathbb{P}}{\mathrm{argmin}} \left( P C \right)
$$</span></p>
<p>Recall that a permutation matrix has binary entries, and exactly one <span class="math inline">1</span> per row.</p>
<p>Finding the optimal permutation matrix <span class="math inline"><em>P</em><sup>‚ãÜ</sup></span> is a combinatorial optimization problem that has well known polynomial-time solution algorithms, e.g.¬†Munkres that runs in <span class="math inline"><em>O</em>(<em>n</em><sup>3</sup>)</span> time.</p>
<p>The assignment problem is a special case of the larger family of <em>optimal transport</em> problems, which mathematicians started looking at in the 18th century.</p>
<h1 id="from-discrete-to-continuous">From discrete to continuous</h1>
<p>The Birkhoff-von Neumann theorem states that, in dimension <span class="math inline"><em>n</em></span>, the set <span class="math inline">ùîπ</span> of doubly stochastic matrices [1] is the convex hull of the set of <span class="math inline"><em>n</em>‚ÄÖ√ó‚ÄÖ<em>n</em></span> permutation matrices. Informally, there is a convex, continuous region of space ‚Äúbetween‚Äù the permutation matrices of a given dimensionality. This convex set is called the Birkhoff polytope [2].</p>
<p>Can we perhaps use this result to solve the assignment problem with a convex, interior point approach?</p>
<p>We can rewrite the assignment problem such that the optimization variable ranges over the Birkhoff polytope; this rewritten form is equivalent to the original one since the cost function is linear in the argument <span class="math inline"><em>P</em></span>, so we expect the optimum to lie at a vertex of the admissible region <span class="math inline">ùîπ</span> (i.e.¬†to be a permutation matrix).</p>
<p><span class="math display">$$
P^{\star} = \underset{P \in \mathbb{B}}{\mathrm{argmin}} \left( P C \right)
$$</span></p>
<p>That implicit constraint under the <span class="math inline">argmin</span> looks nasty. How to address it?</p>
<p>In the following we‚Äôll see how to turn this kind of constrained optimization problem into an unconstrained one over an appropriate <em>manifold</em>.</p>
<h2 id="optimization-on-manifolds">Optimization on manifolds</h2>
<p>Informally, a <i>manifold</i> is a version of Euclidean space <span class="math inline">‚Ñù<sup><em>n</em></sup></span> that is only locally flat (unlike regular Euclidean space which is uniformly flat everywhere).</p>
<p>In order to ‚Äúmake progress‚Äù towards a minimum cost region over a manifold, we must define notions of vector addition over curved spaces, in a way.</p>
<p>The main technical devices for moving between <span class="math inline">‚Ñù<sup><em>n</em></sup></span> and a <a href="https://en.wikipedia.org/wiki/Riemannian_manifold">smooth manifold</a> <span class="math inline">ùïÑ</span> are the <em>orthogonal projection</em> from <span class="math inline">ùïÑ</span> to its tangent, and the <em>retraction</em> operation that assigns points on the tangent bundle <span class="math inline"><em>T</em>ùïÑ</span> to <span class="math inline">ùïÑ</span>.</p>
<p>For an introduction to the relevant definitions I found the book and online course <a href="https://www.nicolasboumal.net/book/#lectures">‚ÄúAn introduction to optimization on smooth manifolds‚Äù</a> to be very accessible.</p>
<h2 id="the-manifold-of-doubly-stochastic-matrices">The manifold of doubly stochastic matrices</h2>
<p>Many interesting sets have manifold structure: the sphere, <a href="https://en.wikipedia.org/wiki/Statistical_manifold">the set of probability distributions</a>, the set of positive-definite matrices. You can construct manifolds from products of manifolds, too. With some furious handwaving on my part and by recognizing the similarities (e.g.¬†positivity and unit norm constraint, but I‚Äôll try to dig out a reference) you can convince yourself that DS matrices have manifold structure in turn.</p>
<p>The numerical implementations of the projection and retraction operators are taken from the literature, i.e.¬†a single paper on the topic [4].</p>
<p>As a side note, one of the internal operations to implement the retraction is the <a href="https://en.wikipedia.org/wiki/Sinkhorn%27s_theorem">Sinkhorn-Knopp iteration</a> which has applications elsewhere too (e.g in optimal transport).</p>
<h2 id="first-order-optimization-on-manifolds">First-order optimization on manifolds</h2>
<p>My optimization code is based on <code>torch</code> with some code borrowed from <code>mctorch</code> [3], extended to implement the manifold of doubly-stochastic matrices. In the following I refer to Python modules and line numbers in my implementation at <a href="https://github.com/ocramz/assignment-riemann-opt/tree/a6bf622b77160dac58dc72fdd1ddd036338d23f3">this commit</a> :</p>
<p>Disregarding some implementation details (e.g.¬†how Pytorch handles mutable objects), at every SGD step (<code>rsgd.py</code> line 57), the optimizer follows the textbook definition:</p>
<ol type="1">
<li>computes the Riemann gradient (<code>egrad2rgrad</code>, from <code>parameter.py</code> line 31) via an orthogonal projection of the Euclidean gradient of the cost function onto <span class="math inline"><em>T</em>ùïÑ</span></li>
<li>scales it by the negative learning rate</li>
<li>computes the retraction of the current point along the scaled Riemann gradient, thereby moving to a new point on <span class="math inline">ùïÑ</span>.</li>
</ol>
<p>The doubly-stochastic matrix manifold operations are implemented <a href="https://github.com/ocramz/assignment-riemann-opt/blob/a6bf622b77160dac58dc72fdd1ddd036338d23f3/doublystochastic.py">here</a>.</p>
<h1 id="experiments">Experiments</h1>
<p>We start by generating a cost matrix of rank <span class="math inline"><em>n</em></span>, and computing the optimal assignment with the Munkres algorithm, which provides us with a cost lower bound (<span class="math inline"><em>y</em><sub><em>L</em><em>B</em></sub></span>).
We then initialize the SGD optimizer at a random doubly stochastic matrix, with the elements sampled from the <a href="https://en.wikipedia.org/wiki/Folded_normal_distribution">‚Äúfolded‚Äù normal distribution</a> <span class="math inline">‚à•ùí©(0,‚ÄÜ1)‚à•</span>.
The learning rate is set to 2e-2 (found empirically).</p>
<p><img src="../images/assign_movie_iter-1000_n-10_lr-0.02_1735984842.gif" width="500/"></p>
<p>In the above animation we see the optimal assignment as dashed red edges, superimposed with the temporary solution in blue. As a visual cue, the thickness of the blue edges is proportional to the respective matrix coefficient, and we see the matrix ‚Äúsparsifies‚Äù as it approaches the optimum at a vertex of the Birkhoff set.</p>
<p><img src="../images/assign_opt_gap_iter-1000_n-10_lr-0.02_1735984842.png" width="400/"></p>
<p>As we can see above, the optimality gap between the current and the Munkres cost (<span class="math inline"><em>y</em>‚ÄÖ‚àí‚ÄÖ<em>y</em><sub><em>L</em><em>B</em></sub></span>) converges smoothly to 0 (from most starting points, in my experiments). I still have to characterize some rare cases in which the bound worsens for a while before improving again.</p>
<h1 id="conclusions">Conclusions</h1>
<p>I was quite surprised this approach works so well, to be honest. Besides usual academic concerns of performance, convergence bounds etc., it would be interesting to generalize this approach and look at the actual connections with optimal transport: what if we have two histograms or <em>distributions</em> rather than the sets <span class="math inline"><em>U</em></span> and <span class="math inline"><em>V</em></span>?</p>
<h2 id="code-repo">Code repo</h2>
<p>All scripts can be found on my GitHub profile <a href="https://github.com/ocramz/assignment-riemann-opt">here</a>.</p>
<h1 id="references">References</h1>
<ol start="0" type="1">
<li>Assignment problem <a href="https://en.wikipedia.org/wiki/Assignment_problem">wikipedia</a></li>
<li>Doubly-stochastic matrix <a href="https://en.wikipedia.org/wiki/Doubly_stochastic_matrix">wikipedia</a></li>
<li>Birkhoff polytope <a href="https://en.wikipedia.org/wiki/Birkhoff_polytope">wikipedia</a></li>
<li><code>mctorch</code> <a href="github">https://github.com/mctorch/mctorch</a></li>
<li>Douik, A. and Hassibi, B., Manifold Optimization Over the Set of Doubly Stochastic Matrices: A Second-Order Geometry, 2018 <a href="https://arxiv.org/abs/1802.02628">arXiv</a></li>
<li>Boumal, N. <a href="https://www.nicolasboumal.net/book/#lectures">An introduction to optimization on smooth manifolds</a></li>
</ol>
    </section>
</article>

        </main>

        <footer>
        </footer>

        </div>


    </body>
</html>
