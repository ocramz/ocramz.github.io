I"¿=<h2 id="introduction">Introduction</h2>

<p>A few days ago I stumbled upon a recent line of research that applies an old idea from functional programming languages (continuation passing) to an old idea from numerical computing (automatic differentiation, AD). The result is an elegant algorithm, which remains close to the textbook treatment of reverse-mode AD (‚Äúbackpropagation‚Äù) and could rightly be considered its ‚Äúnatural‚Äù implementation.</p>

<p>In this post I will give an informal but thorough account of the theory and present a library I‚Äôve published that implements it, <a href="https://hackage.haskell.org/package/ad-delcont">ad-delcont</a>.</p>

<h2 id="optimization">Optimization</h2>

<p>From allocating wartime resources at the dawn of digital computing in the 1940s, to fitting the parameters of today‚Äôs gigantic language models, numerical optimization is an ever-present computational challenge. Optimization is nowadays a vast and fascinating subject of applied mathematics and computer science, and there are many excellent introductory texts on it, which I recommend keeping at hand [1,2].</p>

<p><img src="https://ocramz.github.io/images/ad-delcont-gd.png" alt="Gradient descent" /></p>

<p>Many real-world optimization problems require iterative improvement of a set of continuous parameters (a ‚Äúparameter vector‚Äù) with respect to a chosen cost function, and are tackled with some form of gradient descent. The <em>gradient</em> is a vector in parameter space that points to the direction of fastest increase in the function at a given point. The picture above shows a two-parameter cost function \(J(\theta_0, \theta_1)\) evaluated at a fine grid of points and a possible evolution of the gradient descent algorithm. It‚Äôs worth stressing that many practical cost functions are very costly to evaluate, and often have many more parameters, making full visualization impossible in all but toy examples such as the one above. Computing the gradient of a cost function implemented as a computer program is then a fundamental and ubiquitous task.</p>

<h2 id="differentiating-computer-programs">Differentiating computer programs</h2>

<p>Automatic differentiation is a family of techniques that compute the gradient of computer programs, given a program that computes the cost function of interest. This is achieved in two major ways, i.e. while the user program is compiled or as it runs. Source code transformation is an interesting approach that has yielded many successful implementations (from ADIFOR [3] to Jax [4]) but in this post I will focus on the latter formulation, for the sake of brevity.</p>

<p>I emphasize ‚Äúcomputer programs‚Äù because these contain control structures such as conditionals (<code class="language-plaintext highlighter-rouge">if</code> .. <code class="language-plaintext highlighter-rouge">then</code> .. <code class="language-plaintext highlighter-rouge">else</code>), loops (<code class="language-plaintext highlighter-rouge">while</code>) and numerous other features which do not appear in mathematical notation and must be accounted for in a dedicated way.</p>

<p>A numerical program is usually built up using the syntactic rules of the host language from a library of elementary functional building blocks (e.g. implementations of the exponential or sine function, <code class="language-plaintext highlighter-rouge">exp()</code>, <code class="language-plaintext highlighter-rouge">sin()</code>, and so on). This means that computing the overall sensitivity of this program must involve applying the (multivariate) chain rule of differentiation, while accounting for the program‚Äôs control flow as outlined above.</p>

<p>At this point, a practitioner is faced with multiple implementation details, summarised in this diagram (from [5]) :</p>

<p><img src="https://ocramz.github.io/images/ad-delcont-overview.png" alt="Differentiation of mathematical code" /></p>

<p>Interested readers might want to read [5] for a thorough overview of the design choices that go into an AD system (symbolic vs. numerical vs. algorithmic, forward vs. reverse, etc.).</p>

<h2 id="the-chain-rule">The chain rule</h2>

<p>Suppose we have a composite function \(z(x, y)\), with \(x(u, v)\) and \(y(u, v)\), in which all components are differentiable at least once. The dependencies between these variables can be drawn as a directed acyclic graph :</p>

<p><img src="https://ocramz.github.io/images/ad-delcont-multi-chain-rule.png" alt="Multivariate chain rule" width="400" /></p>

<p>Image from <a href="http://www.math.ucsd.edu/~gptesler/20c/slides/20c_chainrule_f18-handout.pdf">these slides</a>.</p>

<p>The sensitivity of output variable \(z\) to input variable \(v\) must account for all the possible paths taken while ‚Äútraversing‚Äù from \(v\) to \(z\), i.e. while applying the functions at the intermediate tree nodes to their arguments. The multivariate chain rule tells us to sum these contributions : \(\partial_v z = \partial_v x \cdot \partial_x z + \partial_v y \cdot \partial_y z\).</p>

<h2 id="forward-and-reverse">Forward and Reverse</h2>

<p>Ignoring for the sake of exposition all AD approaches that rely on source code analysis and transformation, there remain essentially <em>two</em> ways of computing the derivative of a composite function via ‚Äúnon-standard‚Äù evaluation (NSE). By NSE here we mean augmenting the expression variables with adjoint values (thus computing with ‚Äúdual numbers‚Äù [6], i.e. a first-order Taylor approximation of the expression) and potentially modifying the program execution flow in order to accumulate these adjoints (the sensitivities we‚Äôre interested in). This might sound esoteric but it‚Äôs actually pretty straightforward as I hope I‚Äôll be able to show you.</p>

<p><em>Forward-mode AD</em> is the more intuitive of the two approaches : in this case both the expression value(s) at any intermediate expression node \(v_j\) and the adjoints \(\partial_{x_i} v_j\) are computed in the natural reduction order of the expression: by applying function values to their input arguments. Reduction of functions of dual values follows the familiar rules of derivative calculus. The algorithm computes one partial derivative at a time, by setting the dual part of the variable of interest to 1 and all others to 0. Once the expression is fully reduced, \(\partial_{x_i} z\) can be read off the dual part of the result. The computational cost of this algorithm is one full expression evaluation per input variable.</p>

<p><em>Reverse-mode AD</em> achieves the same result by tracking the reduction order while reducing the expression (‚Äúforward‚Äù), initializing all duals to \(0\), and accumulating the adjoints ‚Äúbackwards‚Äù from the output variable \(z\), which is initialized with the trivial adjoint \(\partial_z z = 1\). Each expression node represents a function, and it is augmented (‚Äúoverloaded‚Äù, in programming language terminology) with a ‚Äúpullback‚Äù [7] that computes how input sensitivities change as a function of output sensitivities. Upon returning to a given expression node \(v_i\), its adjoints are summed over (following the multivariate chain rule shown above). In this case, all parameter sensitivities are computed at once at the end of the backwards sweep. The cost of reverse-mode AD is two full expression evaluations per <em>output</em> variable, which might save a lot of work when applied to expressions with many more input variables, as often happens in optimization and machine learning.</p>

<p>Both AD modes have a long history of successful implementations. Many implementations of reverse-mode AD ‚Äúreify‚Äù the user function into a data structure that tracks the execution history and the functional dependencies (a ‚ÄúWengert tape‚Äù), in order to play the program backwards when accumulating the adjoints.</p>

<p>It turns out, a computational tape is not the only available option for inverting control flow, in sufficiently advanced programming languages. Read on !</p>

<h2 id="reverse-mode-ad-with-delimited-continuations">Reverse-mode AD with delimited continuations</h2>

<p>The long introduction was meant to set the stage for two short code snippet that originally appeared in [8].</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">NumR</span> <span class="o">(</span><span class="k">val</span> <span class="nv">x</span><span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="k">var</span> <span class="n">d</span><span class="k">:</span> <span class="kt">Double</span><span class="o">)</span> <span class="o">{</span>
  <span class="k">def</span> <span class="nf">+</span> <span class="o">(</span><span class="n">that</span><span class="k">:</span> <span class="kt">NumR</span><span class="o">)</span> <span class="k">=</span> <span class="n">shift</span> <span class="o">{</span> <span class="o">(</span><span class="n">k</span><span class="k">:</span> <span class="kt">NumR</span> <span class="o">=&gt;</span> <span class="nc">Unit</span><span class="o">)</span> <span class="k">=&gt;</span>
    <span class="k">val</span> <span class="nv">y</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">NumR</span><span class="o">(</span><span class="k">this</span><span class="o">.</span><span class="py">x</span> <span class="o">+</span> <span class="nv">that</span><span class="o">.</span><span class="py">x</span><span class="o">,</span> <span class="mf">0.0</span><span class="o">);</span>
    <span class="nf">k</span><span class="o">(</span><span class="n">y</span><span class="o">);</span>
    <span class="k">this</span><span class="o">.</span><span class="py">d</span> <span class="o">+=</span> <span class="nv">y</span><span class="o">.</span><span class="py">d</span><span class="o">;</span> 
    <span class="nv">that</span><span class="o">.</span><span class="py">d</span> <span class="o">+=</span> <span class="nv">y</span><span class="o">.</span><span class="py">d</span>
  <span class="o">}</span></code></pre></figure>

<p>This is a Scala implementation of an ‚Äúoverloaded‚Äù summing function over dual numbers that relies on delimited continuations to achieve non-local control flow and specify what to do when a continuation returns. My Scala is pretty rusty so this has been a head scratcher for a while. I‚Äôll first document how my train of thought went while reading this code, and then try to break it down more formally.</p>

<p>1) First we declare a dual number type <code class="language-plaintext highlighter-rouge">NumR</code>, which has fields <code class="language-plaintext highlighter-rouge">.x</code> and <code class="language-plaintext highlighter-rouge">.d</code> for the primal and adjoint respectively.</p>

<p>2) The implementation of the <code class="language-plaintext highlighter-rouge">+</code> method is bracketed within a mysterious <code class="language-plaintext highlighter-rouge">shift</code> higher-order function, which declares a continuation <code class="language-plaintext highlighter-rouge">k</code>, to be used later.</p>

<p>3) A temporary variable <code class="language-plaintext highlighter-rouge">y</code> is declared, having 0 dual value.</p>

<p>4) <code class="language-plaintext highlighter-rouge">k</code> is then applied to <code class="language-plaintext highlighter-rouge">y</code>, and the return value of <code class="language-plaintext highlighter-rouge">k</code> is discarded (?!). This must mean that <code class="language-plaintext highlighter-rouge">y</code> itself is mutated within the execution of <code class="language-plaintext highlighter-rouge">k</code>.</p>

<p>5) Upon returning from <code class="language-plaintext highlighter-rouge">k</code>, the dual part of the mutated value of <code class="language-plaintext highlighter-rouge">y</code> is used to update by accumulation (see multivariate chain rule section above) the dual parts of the input variables <code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">y</code>.</p>

<p>The other interesting snippet is where the adjoint accumulation process kicks off and the gradient is redurned:</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">def</span> <span class="nf">grad</span><span class="o">(</span><span class="n">f</span><span class="k">:</span> <span class="kt">NumR</span> <span class="o">=&gt;</span> <span class="nc">NumR</span> <span class="nd">@cps</span><span class="o">[</span><span class="kt">Unit</span><span class="o">]</span> <span class="o">)(</span><span class="n">x</span><span class="k">:</span> <span class="kt">Double</span><span class="o">)</span> <span class="k">=</span> <span class="o">{</span>
  <span class="k">val</span> <span class="nv">z</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">NumR</span><span class="o">(</span><span class="n">x</span><span class="o">,</span> <span class="mf">0.0</span><span class="o">)</span>
  <span class="n">reset</span>  <span class="o">{</span> 
    <span class="nf">f</span><span class="o">(</span><span class="n">z</span><span class="o">).</span><span class="py">d</span> <span class="k">=</span> <span class="mf">1.0</span> <span class="o">}</span>
  <span class="nv">z</span><span class="o">.</span><span class="py">d</span>
  <span class="o">}</span></code></pre></figure>

<p>1) <code class="language-plaintext highlighter-rouge">grad</code> is a higher-order function that takes the function to be differentiated as a parameter (<code class="language-plaintext highlighter-rouge">f: NumR =&gt; NumR</code>, overloaded to act upon dual numbers <code class="language-plaintext highlighter-rouge">NumR</code>), and an evaluation point <code class="language-plaintext highlighter-rouge">x</code>.</p>

<p>2) A temporary variable <code class="language-plaintext highlighter-rouge">z</code> is declared, having 0 adjoint part and primal part corresponding to the point of interest <code class="language-plaintext highlighter-rouge">x</code>.</p>

<p>3) Within another mysterious bracket <code class="language-plaintext highlighter-rouge">reset</code>, the function <code class="language-plaintext highlighter-rouge">f</code> is evaluated at <code class="language-plaintext highlighter-rouge">z</code></p>

<h2 id="delimited-continuations">Delimited continuations</h2>

<h2 id="ad-delcont">ad-delcont</h2>

<h2 id="references">References</h2>

<p>[1] Nocedal, Wright - Numerical Optimization</p>

<p>[2] Boyd, Vanderberghe - Convex Optimization - <a href="https://web.stanford.edu/~boyd/cvxbook/">https://web.stanford.edu/~boyd/cvxbook/</a></p>

<p>[3] ADIFOR - <a href="https://www.anl.gov/partnerships/adifor-automatic-differentiation-of-fortran-77">https://www.anl.gov/partnerships/adifor-automatic-differentiation-of-fortran-77</a></p>

<p>[4] Jax - <a href="https://github.com/google/jax">https://github.com/google/jax</a></p>

<p>[5] Baydin, Pearlmutter - Automatic differentiation of machine learning algorithms - <a href="https://arxiv.org/abs/1404.7456">https://arxiv.org/abs/1404.7456</a></p>

<p>[6] Shan - Differentiating regions - <a href="http://conway.rutgers.edu/~ccshan/wiki/blog/posts/Differentiation/">http://conway.rutgers.edu/~ccshan/wiki/blog/posts/Differentiation/</a></p>

<p>[7] Innes - Don‚Äôt unroll adjoint : Differentiating SSA-form programs - <a href="https://arxiv.org/abs/1810.07951">https://arxiv.org/abs/1810.07951</a></p>

<p>[8] Wang, Rompf - A Language and Compiler View on Differentiable Programming - ICLR 2018 <a href="https://openreview.net/forum?id=SJxJtYkPG">https://openreview.net/forum?id=SJxJtYkPG</a></p>
:ET