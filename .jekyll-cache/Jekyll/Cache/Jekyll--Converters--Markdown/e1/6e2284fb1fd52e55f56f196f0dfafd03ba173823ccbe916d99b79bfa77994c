I"ƒ'<h2 id="introduction">Introduction</h2>

<p>A few days ago I stumbled upon a recent line of research that applies an old idea from functional programming languages (continuation passing) to an old idea from numerical computing (automatic differentiation, AD). The result is an elegant algorithm, which remains close to the textbook treatment of reverse-mode AD (‚Äúbackpropagation‚Äù) and could rightly be considered its ‚Äúnatural‚Äù implementation.</p>

<p>In this post I will give an informal account of the theory and present a library I‚Äôve published that implements it, <a href="https://hackage.haskell.org/package/ad-delcont">ad-delcont</a>.</p>

<h2 id="automatic-differentiation">Automatic differentiation</h2>

<p>From allocating wartime resources at the dawn of digital computing in the 1940s, to fitting the parameters of today‚Äôs gigantic language models, numerical optimization is an ever-present computational challenge. Optimization is nowadays a vast and fascinating subject of applied mathematics and computer science, and there are many excellent introductory texts on it, which I recommend keeping at hand [1,2].</p>

<p><img src="https://ocramz.github.io/images/ad-delcont-gd.png" alt="Gradient descent" /></p>

<p>Many real-world optimization problems require iterative approximation of a set of continuous parameters (a ‚Äúparameter vector‚Äù), and are tackled with some form of gradient descent. The <em>gradient</em> is a vector in parameter space that points to the direction of fastest increase in the function at a given point. Computing the gradient of a cost function implemented as a computer program is then a fundamental and ubiquitous task.</p>

<p>Automatic differentiation is a family of techniques that compute the gradient of computer programs, given a program that computes the cost function of interest. This is achieved in two major ways, i.e. while the user program is compiled or as it runs. Source code transformation is an interesting approach that has yielded many successful implementations (from ADIFOR [3] to Jax [4]) but in this post I will focus on the latter formulation, for the sake of brevity.</p>

<h2 id="differentiating-computer-programs">Differentiating computer programs</h2>

<p>I emphasize ‚Äúcomputer programs‚Äù because these contain control structures such as conditionals (<code class="language-plaintext highlighter-rouge">if</code> .. <code class="language-plaintext highlighter-rouge">then</code> .. <code class="language-plaintext highlighter-rouge">else</code>), loops (<code class="language-plaintext highlighter-rouge">while</code>) and numerous other features which do not appear in mathematical notation and must be accounted for in a dedicated way.</p>

<p>A numerical program is usually built up using the syntactic rules of the host language from a library of elementary functional building blocks (e.g. implementations of the exponential or sine function, <code class="language-plaintext highlighter-rouge">exp()</code>, <code class="language-plaintext highlighter-rouge">sin()</code>, and so on). This means that computing the overall sensitivity of this program must involve applying the (multivariate) chain rule of differentiation, while accounting for the program‚Äôs control flow as outlined above.</p>

<p>At this point, a practitioner is faced with multiple implementation details, summarised in this diagram (from [5]) :</p>

<p><img src="https://ocramz.github.io/images/ad-delcont-overview.png" alt="Differentiation of mathematical code" /></p>

<p>Interested readers might want to read [5] for a thorough overview of the design choices that go into an AD system (symbolic vs. numerical vs. algorithmic, forward vs. reverse, etc.).</p>

<h2 id="the-chain-rule">The chain rule</h2>

<p>Suppose we have a composite function \(z(x, y)\), with \(x(u, v)\) and \(y(u, v)\), in which all components are differentiable at least once. The dependencies between these variables can be drawn as a directed acyclic graph :</p>

<p><img src="https://ocramz.github.io/images/ad-delcont-multi-chain-rule.png" alt="Multivariate chain rule" width="400" /></p>

<p>Image from <a href="http://www.math.ucsd.edu/~gptesler/20c/slides/20c_chainrule_f18-handout.pdf">these slides</a>.</p>

<p>The sensitivity of output variable \(z\) to input variable \(v\) must account for all the possible paths taken while ‚Äútraversing‚Äù from \(v\) to \(z\), i.e. while applying the functions at the intermediate tree nodes to their arguments. The multivariate chain rule tells us to sum these contributions : \(\partial_v z = \partial_v x \cdot \partial_x z + \partial_v y \cdot \partial_y z\).</p>

<h2 id="forward-and-reverse">Forward and Reverse</h2>

<p>Ignoring for the sake of exposition all AD approaches that rely on source code analysis and transformation, there remain essentially <em>two</em> ways of computing the derivative of a composite function via ‚Äúnon-standard‚Äù evaluation (NSE). By NSE here we mean augmenting the expression variables with adjoint values (thus computing with ‚Äúdual numbers‚Äù [6], i.e. a first-order Taylor approximation of the expression) and potentially modifying the program execution flow in order to accumulate these adjoints (the sensitivities we‚Äôre interested in). This might sound esoteric but it‚Äôs actually pretty straightforward as I hope I‚Äôll be able to show you.</p>

<p><em>Forward-mode AD</em> is the more intuitive of the two approaches : in this case both the expression value(s) at any intermediate expression node \(v_j\) and the adjoints \(\partial_{x_i} v_j\) are computed in the natural reduction order of the expression: by applying function values to their input arguments. Reduction of functions of dual values follows the familiar rules of derivative calculus. The algorithm computes one partial derivative at a time, by setting the dual part of the variable of interest to 1 and all others to 0. Once the expression is fully reduced, \(\partial_{x_i} z\) can be read off the dual part of the result. The computational cost of this algorithm is one full expression evaluation per input variable.</p>

<p><em>Reverse-mode AD</em> achieves the same result by tracking the reduction order while reducing the expression (‚Äúforward‚Äù), initializing all duals to \(0\), and accumulating the adjoints ‚Äúbackwards‚Äù from the output variable \(z\), which is initialized with the trivial adjoint \(\partial_z z = 1\). Each expression node represents a function, and it is augmented (‚Äúoverloaded‚Äù, in programming language terminology) with a ‚Äúpullback‚Äù [7] that computes how input sensitivities change as a function of output sensitivities. Upon returning to a given expression node \(v_i\), its adjoints are summed over (following the multivariate chain rule shown above). In this case, all parameter sensitivities are computed at once at the end of the backwards sweep. The cost of reverse-mode AD is two full expression evaluations per <em>output</em> variable, which might save a lot of work when applied to expressions with many more input variables, as often happens in optimization and machine learning.</p>

<p>Both AD modes have a long history of successful implementations. Many implementations of reverse-mode AD ‚Äúreify‚Äù the user function into a data structure that tracks the execution history and the functional dependencies (a ‚ÄúWengert tape‚Äù), in order to play the program backwards when accumulating the adjoints.</p>

<h2 id="wang-et-al">Wang et al</h2>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">NumR</span> <span class="o">(</span><span class="k">val</span> <span class="nv">x</span><span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="k">var</span> <span class="n">d</span><span class="k">:</span> <span class="kt">Double</span><span class="o">)</span> <span class="o">{</span>
  <span class="k">def</span> <span class="nf">+</span> <span class="o">(</span><span class="n">that</span><span class="k">:</span> <span class="kt">NumR</span><span class="o">)</span> <span class="k">=</span> <span class="n">shift</span> <span class="o">{</span> <span class="o">(</span><span class="n">k</span><span class="k">:</span> <span class="kt">NumR</span> <span class="o">=&gt;</span> <span class="nc">Unit</span><span class="o">)</span> <span class="k">=&gt;</span>
    <span class="k">val</span> <span class="nv">y</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">NumR</span><span class="o">(</span><span class="k">this</span><span class="o">.</span><span class="py">x</span> <span class="o">+</span> <span class="nv">that</span><span class="o">.</span><span class="py">x</span><span class="o">,</span> <span class="mf">0.0</span><span class="o">);</span>
    <span class="nf">k</span><span class="o">(</span><span class="n">y</span><span class="o">);</span>
    <span class="k">this</span><span class="o">.</span><span class="py">d</span> <span class="o">+=</span> <span class="nv">y</span><span class="o">.</span><span class="py">d</span><span class="o">;</span> 
    <span class="nv">that</span><span class="o">.</span><span class="py">d</span> <span class="o">+=</span> <span class="nv">y</span><span class="o">.</span><span class="py">d</span>
  <span class="o">}</span></code></pre></figure>

<h2 id="delimited-continuations">Delimited continuations</h2>

<h2 id="ad-delcont">ad-delcont</h2>

<h2 id="references">References</h2>

<p>[1] Nocedal, Wright - Numerical Optimization</p>

<p>[2] Boyd, Vanderberghe - Convex Optimization - https://web.stanford.edu/~boyd/cvxbook/</p>

<p>[3] ADIFOR - https://www.anl.gov/partnerships/adifor-automatic-differentiation-of-fortran-77</p>

<p>[4] Jax - https://github.com/google/jax</p>

<p>[5] Baydin, Pearlmutter - Automatic differentiation of machine learning algorithms - https://arxiv.org/abs/1404.7456</p>

<p>[6] Shan - Differentiating regions - http://conway.rutgers.edu/~ccshan/wiki/blog/posts/Differentiation/</p>

<p>[7] Innes - Don‚Äôt unroll adjoint : Differentiating SSA-form programs - https://arxiv.org/abs/1810.07951</p>
:ET