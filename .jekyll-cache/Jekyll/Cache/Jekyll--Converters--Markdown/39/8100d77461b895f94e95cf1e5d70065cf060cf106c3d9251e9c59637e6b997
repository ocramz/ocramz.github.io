I"Î<h2 id="introduction">Introduction</h2>

<p>A few days ago I stumbled upon a recent line of research that applies an old idea from functional programming languages (continuation passing) to an old idea from numerical computing (automatic differentiation, AD). The result is an elegant algorithm, which remains close to the textbook treatment of reverse-mode AD (‚Äúbackpropagation‚Äù) and could rightly be considered its ‚Äúnatural‚Äù implementation.</p>

<p>In this post I will briefly introduce the theory and present a library I‚Äôve published that implements it, <a href="https://hackage.haskell.org/package/ad-delcont">ad-delcont</a>.</p>

<h2 id="automatic-differentiation">Automatic differentiation</h2>

<p>From allocating wartime resources at the dawn of digital computing in the 1940s, to fitting the parameters of today‚Äôs gigantic language models, numerical optimization is an ever-present computational challenge. Optimization is nowadays a vast and fascinating subject of applied mathematics and computer science, and there are many excellent introductory texts on it, which I recommend keeping at hand [1,2].</p>

<p>Many real-world optimization problems require iterative approximation of a set of continuous parameters (a ‚Äúparameter vector‚Äù), and are tackled with some form of gradient descent. The <em>gradient</em> is a vector in parameter space that points to the direction of fastest increase in the function at a given point. Computing the gradient of a cost function implemented as a computer program is then a fundamental and ubiquitous task.</p>

<p>Automatic differentiation is a family of techniques that compute the gradient of computer programs, given a program that computes the cost function of interest. This is achieved in two major ways, i.e. while the user program is compiled or as it runs. Source code transformation is an interesting approach that has yielded many successful implementations (from ADIFOR [3] to PyTorch [4]) but in this post I will focus on the latter formulation, for the sake of brevity.</p>

<h2 id="differentiating-computer-programs">Differentiating computer programs</h2>

<p>I emphasize ‚Äúcomputer programs‚Äù because these contain control structures such as conditionals (‚Äúif .. then .. else‚Äù), loops (‚Äúwhile‚Äù) and numerous other features which do not appear in mathematical notation and must be accounted for in a dedicated way.</p>

<p>A numerical program is usually built up using the syntactic rules of the host language from a library of elementary functional building blocks (e.g. implementations of the exponential or sine function, <code class="language-plaintext highlighter-rouge">exp()</code>, <code class="language-plaintext highlighter-rouge">sin()</code>, and so on). This means that computing the overall sensitivity of this program must involve applying the (multivariate) chain rule of differentiation, while accounting for the program‚Äôs control flow as outlined above.</p>

<h2 id="the-chain-rule">The chain rule</h2>

<p>Suppose we have a simple function \(z(x, y)\), with \(x(u, v)\) and \(y(u, v)\)</p>

<p><img src="https://ocramz.github.io/images/ad-delcont-multi-chain-rule.png" alt="Multivariate chain rule" width="400" /></p>

<p>Image from <a href="http://www.math.ucsd.edu/~gptesler/20c/slides/20c_chainrule_f18-handout.pdf">these slides</a>.</p>

<h2 id="wang-et-al">Wang et al</h2>

<h2 id="delimited-continuations">Delimited continuations</h2>

<h2 id="ad-delcont">ad-delcont</h2>

<h2 id="references">References</h2>

<p>[1] Nocedal, Wright - Numerical Optimization</p>

<p>[2] Boyd, Vanderberghe - Convex Optimization - https://web.stanford.edu/~boyd/cvxbook/</p>

<p>[3] ADIFOR - https://www.anl.gov/partnerships/adifor-automatic-differentiation-of-fortran-77</p>

<p>[4] PyTorch - https://pytorch.org/</p>
:ET